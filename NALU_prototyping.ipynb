{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAC(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.W_hat = nn.Parameter(torch.Tensor(self.out_dim, self.in_dim))\n",
    "        self.M_hat = nn.Parameter(torch.Tensor(self.out_dim, self.in_dim))\n",
    "        nn.init.xavier_normal_(self.W_hat)\n",
    "        nn.init.xavier_normal_(self.M_hat)\n",
    "        self.bias = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W = torch.tanh(self.W_hat) * torch.sigmoid(self.M_hat)\n",
    "        return F.linear(x, W, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NALU(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "#         self.G = nn.Linear(self.in_dim,\n",
    "#                            self.out_dim,\n",
    "#                            bias=False)\n",
    "        self.G = nn.Parameter(torch.Tensor(1, 1))\n",
    "        nn.init.xavier_normal_(self.G)\n",
    "        self.nac = NAC(self.in_dim, self.out_dim)\n",
    "        self.eps = 1e-10\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.nac(x)\n",
    "        g = torch.sigmoid(self.G)\n",
    "        m = self.nac(torch.log(torch.abs(x) + self.eps))\n",
    "        m = torch.exp(m)\n",
    "        y = (g * a) + (1 - g) * m\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-12\n",
    "X_train = np.random.uniform(-5, 5+eps, size=(2000, 2))\n",
    "Y_train = X_train[:, 0] * X_train[:, 1]\n",
    "\n",
    "X_valid = np.random.uniform(-5, 5+eps, size=(500, 2))\n",
    "Y_valid = X_valid[:, 0] * X_valid[:, 1]\n",
    "\n",
    "X_test = np.random.uniform(-50, 50+eps, size=(2000, 2))\n",
    "Y_test = X_test[:, 0] * X_test[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, target, batch_size, mode='test', use_gpu=False):\n",
    "    idx = np.arange(0, data.shape[0])\n",
    "    \n",
    "    if mode == 'train':\n",
    "        np.random.shuffle(idx)\n",
    "\n",
    "    while idx.shape[0] > 0:\n",
    "        batch_idx = idx[:batch_size]\n",
    "        idx = idx[batch_size:]\n",
    "        batch_data = data[batch_idx]\n",
    "        batch_target = target[batch_idx]\n",
    "        \n",
    "        batch_data = torch.from_numpy(batch_data).float()\n",
    "        batch_target = torch.from_numpy(batch_target).float().view(-1, 1)\n",
    "        \n",
    "        if use_gpu:\n",
    "            batch_data = batch_data.cuda()\n",
    "            batch_target = batch_target.cuda()\n",
    "        \n",
    "        yield batch_data, batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_loss(model, criterion, data, targets, use_gpu=False):\n",
    "    preds, targets = get_eval_preds(model, data, targets, use_gpu)\n",
    "    loss = criterion(preds, targets)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_preds(model, data, targets, use_gpu=False):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model_preds = []\n",
    "        tensor_targets = []\n",
    "        for x, y in get_batches(X_valid, Y_valid, batch_size,\n",
    "                                mode='test', use_gpu=use_gpu):\n",
    "            model_preds.append(model(x))\n",
    "            tensor_targets.append(y)\n",
    "        model_preds = torch.cat(model_preds, dim=0)\n",
    "        tensor_targets = torch.cat(tensor_targets, dim=0)\n",
    "    return model_preds, tensor_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after epoch 0: 5.484178066253662\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 1: 5.477092742919922\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 2: 5.475063323974609\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 3: 5.471747398376465\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 4: 5.469732761383057\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 5: 5.467689037322998\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 6: 5.465944766998291\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 7: 5.463921070098877\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 8: 5.462299346923828\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 9: 5.460833549499512\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 10: 5.459917068481445\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 11: 5.457761287689209\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 12: 5.456937313079834\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 13: 5.455726623535156\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 14: 5.453793048858643\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 15: 5.452897548675537\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 16: 5.453068733215332\n",
      "Validation loss after epoch 17: 5.450931072235107\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 18: 5.449306964874268\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 19: 5.4473490715026855\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 20: 5.447644233703613\n",
      "Validation loss after epoch 21: 5.4459147453308105\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 22: 5.446629047393799\n",
      "Validation loss after epoch 23: 5.4442458152771\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 24: 5.442931652069092\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 25: 5.442986488342285\n",
      "Validation loss after epoch 26: 5.442498683929443\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 27: 5.441523551940918\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 28: 5.440990924835205\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 29: 5.441040515899658\n",
      "Validation loss after epoch 30: 5.43988561630249\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 31: 5.439301490783691\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 32: 5.439486980438232\n",
      "Validation loss after epoch 33: 5.43858528137207\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 34: 5.437991142272949\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 35: 5.438276767730713\n",
      "Validation loss after epoch 36: 5.437806606292725\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 37: 5.439502716064453\n",
      "Validation loss after epoch 38: 5.437300205230713\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 39: 5.436685085296631\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 40: 5.436777114868164\n",
      "Validation loss after epoch 41: 5.436191558837891\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 42: 5.436059474945068\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 43: 5.436100482940674\n",
      "Validation loss after epoch 44: 5.435879230499268\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 45: 5.435724258422852\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 46: 5.435800552368164\n",
      "Validation loss after epoch 47: 5.435967445373535\n",
      "Validation loss after epoch 48: 5.435867786407471\n",
      "Validation loss after epoch 49: 5.4355244636535645\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 50: 5.4350266456604\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 51: 5.434469699859619\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 52: 5.43437385559082\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 53: 5.434035778045654\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 54: 5.43411922454834\n",
      "Validation loss after epoch 55: 5.433952808380127\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 56: 5.434131622314453\n",
      "Validation loss after epoch 57: 5.4342851638793945\n",
      "Validation loss after epoch 58: 5.433889389038086\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 59: 5.43370246887207\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 60: 5.434141159057617\n",
      "Validation loss after epoch 61: 5.43513822555542\n",
      "Validation loss after epoch 62: 5.434558391571045\n",
      "Validation loss after epoch 63: 5.434610843658447\n",
      "Validation loss after epoch 64: 5.4344024658203125\n",
      "Ran out of patience, early stopping employed!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "patience = 5\n",
    "running_patience = 5\n",
    "checkpoint = 'best_model.sav'\n",
    "print_every = 200\n",
    "num_epochs = 2000\n",
    "running_batch = 0\n",
    "running_loss = 0\n",
    "min_loss = float('inf')\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "criterion = nn.SmoothL1Loss()\n",
    "\n",
    "model = NALU(2, 1)\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "optimizer = SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for x, y in get_batches(X_train, Y_train, batch_size,\n",
    "                            mode='train', use_gpu=use_gpu):\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        running_batch += 1\n",
    "        \n",
    "#         if running_batch % print_every == 0:\n",
    "#             print('Training loss after {} batches: {}'.format(running_batch, running_loss/running_batch))\n",
    "            \n",
    "    valid_loss = get_eval_loss(model, criterion,\n",
    "                              X_valid, Y_valid, False)\n",
    "    print(\"Validation loss after epoch {}: {}\".format(epoch, valid_loss))\n",
    "    if valid_loss < min_loss:\n",
    "        min_loss = valid_loss\n",
    "        print('Validation loss improved! Saving model.')\n",
    "        with open(checkpoint, 'wb') as f:\n",
    "            torch.save(model.state_dict(), f)\n",
    "            running_patience = patience\n",
    "    else:\n",
    "        running_patience -= 1\n",
    "    if running_patience == 0:\n",
    "        print('Ran out of patience, early stopping employed!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = get_eval_loss(model, criterion,\n",
    "                          X_test, Y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.807424545288086"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds, test_targets = get_eval_preds(model, X_test, Y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = test_preds.cpu().numpy().flatten()\n",
    "test_targets = test_targets.cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = np.isclose(test_preds, test_targets, rtol=1e-4).astype(np.int32).mean()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2643, 0.1579]], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tanh(model.nac.W_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7959, 0.4905]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(model.nac.M_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[3.0545, 0.0673]], requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.G.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
