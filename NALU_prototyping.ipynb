{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from models.nac import NAC\n",
    "from models.nalu import NALU\n",
    "from torch.optim import Adam, SGD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NAC(nn.Module):\n",
    "#     def __init__(self, in_dim, out_dim):\n",
    "#         super().__init__()\n",
    "#         self.in_dim = in_dim\n",
    "#         self.out_dim = out_dim\n",
    "#         self.W_hat = nn.Parameter(torch.Tensor(self.out_dim, self.in_dim))\n",
    "#         self.M_hat = nn.Parameter(torch.Tensor(self.out_dim, self.in_dim))\n",
    "#         nn.init.xavier_normal_(self.W_hat)\n",
    "#         nn.init.xavier_normal_(self.M_hat)\n",
    "#         self.bias = None\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         W = torch.tanh(self.W_hat) * torch.sigmoid(self.M_hat)\n",
    "#         return F.linear(x, W, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NALU(nn.Module):\n",
    "#     def __init__(self, in_dim, out_dim):\n",
    "#         super().__init__()\n",
    "#         self.in_dim = in_dim\n",
    "#         self.out_dim = out_dim\n",
    "#         self.G = nn.Parameter(torch.Tensor(1, 1))\n",
    "#         nn.init.xavier_normal_(self.G)\n",
    "#         self.nac = NAC(self.in_dim, self.out_dim)\n",
    "#         self.eps = 1e-12\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         a = self.nac(x)\n",
    "#         g = torch.sigmoid(self.G)\n",
    "#         m = self.nac(torch.log(torch.abs(x) + self.eps))\n",
    "#         m = torch.exp(m)\n",
    "#         y = (g * a) + (1 - g) * m\n",
    "#         return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-12\n",
    "X_train = np.random.uniform(-5, 5+eps, size=(2000, 2))\n",
    "Y_train = X_train[:, 0] * X_train[:, 1]\n",
    "\n",
    "X_valid = np.random.uniform(-5, 5+eps, size=(500, 2))\n",
    "Y_valid = X_valid[:, 0] * X_valid[:, 1]\n",
    "\n",
    "X_test = np.random.uniform(-50, 50+eps, size=(2000, 2))\n",
    "Y_test = X_test[:, 0] * X_test[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, target, batch_size, mode='test', use_gpu=False):\n",
    "    idx = np.arange(0, data.shape[0])\n",
    "    \n",
    "    if mode == 'train':\n",
    "        np.random.shuffle(idx)\n",
    "\n",
    "    while idx.shape[0] > 0:\n",
    "        batch_idx = idx[:batch_size]\n",
    "        idx = idx[batch_size:]\n",
    "        batch_data = data[batch_idx]\n",
    "        batch_target = target[batch_idx]\n",
    "        \n",
    "        batch_data = torch.from_numpy(batch_data).float()\n",
    "        batch_target = torch.from_numpy(batch_target).float().view(-1, 1)\n",
    "        \n",
    "        if use_gpu:\n",
    "            batch_data = batch_data.cuda()\n",
    "            batch_target = batch_target.cuda()\n",
    "        \n",
    "        yield batch_data, batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_loss(model, criterion, data, targets, use_gpu=False):\n",
    "    preds, targets = get_eval_preds(model, data, targets, use_gpu)\n",
    "    loss = criterion(preds, targets)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_preds(model, data, targets, use_gpu=False):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model_preds = []\n",
    "        tensor_targets = []\n",
    "        for x, y in get_batches(data, targets, batch_size,\n",
    "                                mode='test', use_gpu=use_gpu):\n",
    "            model_preds.append(model(x))\n",
    "            tensor_targets.append(y)\n",
    "        model_preds = torch.cat(model_preds, dim=0)\n",
    "        tensor_targets = torch.cat(tensor_targets, dim=0)\n",
    "    return model_preds, tensor_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after epoch 0: 5.76000452041626\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 1: 5.758058071136475\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 2: 5.756496429443359\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 3: 5.755018711090088\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 4: 5.753583908081055\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 5: 5.752229690551758\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 6: 5.7509260177612305\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 7: 5.749770641326904\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 8: 5.74862003326416\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 9: 5.74719762802124\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 10: 5.746130466461182\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 11: 5.74494743347168\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 12: 5.7439656257629395\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 13: 5.7426629066467285\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 14: 5.7416815757751465\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 15: 5.740633964538574\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 16: 5.739498138427734\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 17: 5.738234996795654\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 18: 5.736954212188721\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 19: 5.735752105712891\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 20: 5.7344160079956055\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 21: 5.733373165130615\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 22: 5.732264041900635\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 23: 5.731044769287109\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 24: 5.729795932769775\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 25: 5.728631496429443\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 26: 5.727561950683594\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 27: 5.726243019104004\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 28: 5.724920272827148\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 29: 5.7235212326049805\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 30: 5.7223286628723145\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 31: 5.720684051513672\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 32: 5.719578742980957\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 33: 5.718225479125977\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 34: 5.716933250427246\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 35: 5.715463638305664\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 36: 5.714170455932617\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 37: 5.71297550201416\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 38: 5.711543560028076\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 39: 5.710378646850586\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 40: 5.709053993225098\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 41: 5.707606792449951\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 42: 5.706258773803711\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 43: 5.7048139572143555\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 44: 5.703071117401123\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 45: 5.701757907867432\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 46: 5.700582981109619\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 47: 5.699057579040527\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 48: 5.697603225708008\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 49: 5.696352958679199\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 50: 5.695003509521484\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 51: 5.693559646606445\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 52: 5.692340850830078\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 53: 5.690857410430908\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 54: 5.6892476081848145\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 55: 5.688045978546143\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 56: 5.686713695526123\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 57: 5.685243606567383\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 58: 5.684116840362549\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 59: 5.682770729064941\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 60: 5.681270599365234\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 61: 5.680058002471924\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 62: 5.678743839263916\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 63: 5.677373886108398\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 64: 5.67616081237793\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 65: 5.67474365234375\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 66: 5.6736063957214355\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 67: 5.672234058380127\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 68: 5.671130657196045\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 69: 5.669926643371582\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 70: 5.6686930656433105\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 71: 5.667737007141113\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 72: 5.666569232940674\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 73: 5.665491104125977\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 74: 5.664417743682861\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 75: 5.6634368896484375\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 76: 5.662293910980225\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 77: 5.661428451538086\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 78: 5.6604905128479\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 79: 5.659543991088867\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 80: 5.658661365509033\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 81: 5.657657623291016\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 82: 5.65681266784668\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 83: 5.655888080596924\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 84: 5.655187606811523\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 85: 5.654422283172607\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 86: 5.6537604331970215\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 87: 5.652916431427002\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 88: 5.6523613929748535\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 89: 5.651834964752197\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 90: 5.651121616363525\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 91: 5.650665760040283\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 92: 5.649988174438477\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 93: 5.6493353843688965\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 94: 5.648855209350586\n",
      "Validation loss improved! Saving model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after epoch 95: 5.648275375366211\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 96: 5.647966384887695\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 97: 5.647377014160156\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 98: 5.647089004516602\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 99: 5.646607875823975\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 100: 5.646136283874512\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 101: 5.645874500274658\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 102: 5.645546913146973\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 103: 5.645334243774414\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 104: 5.644866466522217\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 105: 5.644613742828369\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 106: 5.644331455230713\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 107: 5.644043922424316\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 108: 5.643730163574219\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 109: 5.6437153816223145\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 110: 5.643499851226807\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 111: 5.643218517303467\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 112: 5.6432600021362305\n",
      "Validation loss after epoch 113: 5.642970085144043\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 114: 5.64262056350708\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 115: 5.6427459716796875\n",
      "Validation loss after epoch 116: 5.642475128173828\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 117: 5.642175674438477\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 118: 5.642154693603516\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 119: 5.641995906829834\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 120: 5.642245292663574\n",
      "Validation loss after epoch 121: 5.64225435256958\n",
      "Validation loss after epoch 122: 5.642259120941162\n",
      "Validation loss after epoch 123: 5.642187595367432\n",
      "Validation loss after epoch 124: 5.641841411590576\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 125: 5.641958713531494\n",
      "Validation loss after epoch 126: 5.641872882843018\n",
      "Validation loss after epoch 127: 5.641773700714111\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 128: 5.641840934753418\n",
      "Validation loss after epoch 129: 5.6417622566223145\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 130: 5.641842842102051\n",
      "Validation loss after epoch 131: 5.641659259796143\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 132: 5.641762733459473\n",
      "Validation loss after epoch 133: 5.641775131225586\n",
      "Validation loss after epoch 134: 5.641633987426758\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 135: 5.641608238220215\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 136: 5.641720771789551\n",
      "Validation loss after epoch 137: 5.641824245452881\n",
      "Validation loss after epoch 138: 5.641747951507568\n",
      "Validation loss after epoch 139: 5.641725063323975\n",
      "Validation loss after epoch 140: 5.641651153564453\n",
      "Validation loss after epoch 141: 5.641709804534912\n",
      "Validation loss after epoch 142: 5.641717910766602\n",
      "Validation loss after epoch 143: 5.641600131988525\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 144: 5.641793727874756\n",
      "Validation loss after epoch 145: 5.641598701477051\n",
      "Validation loss improved! Saving model.\n",
      "Validation loss after epoch 146: 5.641922950744629\n",
      "Validation loss after epoch 147: 5.642024993896484\n",
      "Validation loss after epoch 148: 5.641991138458252\n",
      "Validation loss after epoch 149: 5.64171028137207\n",
      "Validation loss after epoch 150: 5.6419997215271\n",
      "Validation loss after epoch 151: 5.6420698165893555\n",
      "Validation loss after epoch 152: 5.641818523406982\n",
      "Validation loss after epoch 153: 5.642142295837402\n",
      "Validation loss after epoch 154: 5.641868591308594\n",
      "Validation loss after epoch 155: 5.641872406005859\n",
      "Validation loss after epoch 156: 5.641836166381836\n",
      "Validation loss after epoch 157: 5.642051696777344\n",
      "Validation loss after epoch 158: 5.642243385314941\n",
      "Validation loss after epoch 159: 5.6425018310546875\n",
      "Validation loss after epoch 160: 5.641819953918457\n",
      "Ran out of patience, early stopping employed!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "patience = 15\n",
    "running_patience = 5\n",
    "checkpoint = 'best_model.sav'\n",
    "print_every = 200\n",
    "num_epochs = 5000\n",
    "running_batch = 0\n",
    "running_loss = 0\n",
    "min_loss = float('inf')\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "criterion = nn.SmoothL1Loss()\n",
    "\n",
    "model = NALU(2, 1)\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "optimizer = Adam(model.parameters())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for x, y in get_batches(X_train, Y_train, batch_size,\n",
    "                            mode='train', use_gpu=use_gpu):\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        running_batch += 1\n",
    "        \n",
    "#         if running_batch % print_every == 0:\n",
    "#             print('Training loss after {} batches: {}'.format(running_batch, running_loss/running_batch))\n",
    "            \n",
    "    valid_loss = get_eval_loss(model, criterion,\n",
    "                              X_valid, Y_valid, False)\n",
    "    print(\"Validation loss after epoch {}: {}\".format(epoch, valid_loss))\n",
    "    if valid_loss < min_loss:\n",
    "        min_loss = valid_loss\n",
    "        print('Validation loss improved! Saving model.')\n",
    "        with open(checkpoint, 'wb') as f:\n",
    "            torch.save(model.state_dict(), f)\n",
    "            running_patience = patience\n",
    "    else:\n",
    "        running_patience -= 1\n",
    "    if running_patience == 0:\n",
    "        print('Ran out of patience, early stopping employed!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = get_eval_loss(model, criterion,\n",
    "                          X_test, Y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "615.465087890625"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds, test_targets = get_eval_preds(model, X_test, Y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = test_preds.cpu().numpy().flatten()\n",
    "test_targets = test_targets.cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = np.isclose(test_preds, test_targets, rtol=1e-4).astype(np.int32).mean()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4055, -0.1050]], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tanh(model.nac.W_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5964, 0.3760]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(model.nac.M_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2004.8478   , 1297.4943   , -470.1426   , -106.39968  ,\n",
       "       1777.3282   , 1015.44104  ,  770.76166  ,  146.34264  ,\n",
       "          2.3569129,  649.5627   ], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_targets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.102668 , -9.007928 ,  8.301753 ,  2.3614633, 10.970412 ,\n",
       "        6.549237 ,  8.244163 ,  2.596319 ,  0.5792012,  5.3412333],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-47.09977046, -42.56597659],\n",
       "       [ 38.88390069,  33.36841774],\n",
       "       [-42.14832541,  11.15447884],\n",
       "       [-12.55681032,   8.47346406],\n",
       "       [-47.26149487, -37.60626413],\n",
       "       [-25.14039153, -40.39082043],\n",
       "       [-36.60591108, -21.05566128],\n",
       "       [ -9.89400027, -14.7910476 ],\n",
       "       [ -2.00640687,  -1.17469336],\n",
       "       [-20.68032595, -31.40969368]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2004.84772673, 1297.49424143, -470.14260403, -106.39968089,\n",
       "       1777.32825938])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
